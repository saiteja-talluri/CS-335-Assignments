0. Nature of the plot and description of observations

Explantion :

At the start the training accuracy in bagging increases fast compared to that of the boosting. The training accuracies for the boosting keeps on increasing with the number of classifiers whereas that of the bagging algorithm saturates after reaching a certain extent. 

The test accuracy in boosting has constant rise and dips as the algorithm tries to classify the misclassified points but as the number of classifers increases the model overfits so the test accuarcy can attain a saturation. In bagging the test accuarcy increases quickly at the start because of the independent perceptrons but eventually slows down due to repetition in the sampled data.


--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

1. Compare the training accuracies of both the algorithms and explain your observations from a theoretical point of view.

Explanation :

At the start the training accuracy in bagging increases fast compared to that of the boosting. The training accuracies for the boosting keeps on increasing with the number of classifiers whereas that of the bagging algorithm saturates after reaching a certain extent. 

Reasons:

(a) Bagging is a parallel ensemble where each perceptron is trained independently on a randomly sampled data. So in bagging the variance decreases rather than the bias, so even after increasing the classifiers the training accuracy saturates. But at the start since it is parallel the accuracy grows a bit fast.

(b) Boosting is a sequential ensemble where each perceptron is trained to do well on those points which the previous preceptrons failed. So in boosting the bias decreases not the variance. Every perceptron tries to classfiy the misclassified points so far correctly. So the training accuracy keeps on increasing (indeed its boosting). But at the start since its sequential accuracy grows a bit slowly.

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

2. "An ensemble combining perceptrons with weighted majority cannot be represented as an equivalent single perceptron." IS the above statement true? Give proper justification to your claim and accompany with examples if possible.

Explanation :

Yes, the given statement is true. Consider this example taught in the class. Lets say we have 2D data with labels +1 and -1 which is not linearly separable but can be seprated by 3 non concurrent lines i.e., points to the right/left of one line  are labelled +1 and left/right -1. 


Now for this data the single perceptron cannot classify properly, but the ensemble combining perceptrons with weighted majority (weights being non-zero here) can classify the data efficiently by assigning proper weights to the individual perceptrons.


--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
